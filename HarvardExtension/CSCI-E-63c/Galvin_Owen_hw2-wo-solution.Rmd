---
title: 'CSCI E-63C: Week 2 Assignment'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(ggplot2)
library(reshape2)
library(dplyr)
library(readr)
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

One of the first steps in the analysis of a new dataset, often as part of data cleaning, typically involves generation of high level summaries, such as: how many observations, attributes, which ones are predictors and which ones are (could be?) outcomes, what are their ranges, distributions, percentages of missing values, how strongly correlated are the predictors among themselves and with the outcome(s), etc.  It is usually at this stage when we develop our initial intuition about the level of difficulty of the problem and of the challenges presented by this particular dataset and therefore form our first set of ideas as to how to approach it.  There are many multivariate methods under unsupervised learning umbrella that are extremely useful in this setting (that will be introduced later in the course), but first things first, and here we will start by loading few datasets into R and exploring their attributes in the form of univariate summaries and bivariate plots and contingency tables (where applicable).

For this assignment we will use several datasets available from [UCI machine learning repository](http://archive.ics.uci.edu/ml/datasets.html) that for convenience and as to not to depend on UCI ML repository availability have been also copied into this course website. Once you have downloaded them onto your computer, they can be loaded into R using function `read.table` with necessary options (of which most useful/relevant include: `sep` -- defining field separator and `header` -- instructing `read.table` to use fields in the first line as column headers). In principle, `read.table` can also use URL as a full path to the dataset, but here, to be able to work independently of network connection, we recommend that you download those datasets locally and provide `read.table` with appropriate paths to their local copies.  The simplest thing is probably to copy them to the same directory where your .Rmd file is, in which case just the file name passed to `read.table` should suffice.  As always, please remember, that `help(read.table)` (or, `?read.table` as a shorthand) will tell you quite a bit about this function and its parameters.

For those datasets that do not have column names included in their data files, it is often convenient to assign them explicitly. Please note that for some of these datasets categorical variables are encoded in the form of integer values, that by default R will interpret as continuous variables while the behavior of many R functions depends on the type of the input variables.

The code excerpts and their output presented below illustrate some of these most basic steps as applied to one of the datasets available from UCI. The homework problems follow after that -- they will require you to apply similar kind of approaches to generate high levels summaries of few other UCI datasets.

```{r habRead}
habDat <- read.table("./data/haberman.data",sep=",")
colnames(habDat) <- c("age","year","nodes","surv")
summary(habDat$surv)
habDat$surv <- c("yes","no")[habDat$surv]
summary(habDat$surv)
habDat$surv <- factor(habDat$surv)
summary(habDat$surv)
```

The following two examples below show generation of xy-scatterplots of age and node count for the patients in this dataset with color indicating their survival past 5 years using basic plotting capabilities in R as well as those provided by the package `ggplot2`.

```{r habPlot,fig.height=5,fig.width=10}
oldPar <- par(mfrow=c(1:2),ps=16)
for ( iSurv in sort(unique(habDat$surv)) ) {
    plot(habDat[,c("age","nodes")],type="n",
        main=paste("Survival:",iSurv))
    iTmp <- (1:length(levels(habDat$surv)))[levels(habDat$surv)==iSurv]
    points(habDat[habDat$surv==iSurv,c("age","nodes")],col=iTmp,pch=iTmp)
}
par(oldPar)
```

```{r habPlotGG,fig.height=3,fig.width=6}
ggplot(habDat,aes(x=age,y=nodes,colour=surv,shape=surv)) + 
geom_point() + facet_wrap(~surv)
```

It seems that higher number of nodes might be associated with lower probability of survival. One attempt to quantify this relationship might involve testing relationship between indicators of survival and count of nodes exceeding arbitrarily chosen cutoffs (zero or 75th percentile in the example below). There is also substantial degree of overplotting due to integer values of node count and year that might, for instance, make it less apparent that not all patients with zero nodes survive if all of them were plotted in the same panel.  

```{r habTbl}
habDat$nodes0 <- habDat$nodes==0
table(habDat[, c("surv","nodes0")])
habDat$nodes75 <- habDat$nodes>=quantile(habDat$nodes,probs=0.75)
table(habDat[, c("surv","nodes75")])
```

Please feel free to model your solutions after the examples shown above, while exercising necessary judgement as to which attributes are best represented as continuous and which ones -- as categorical, etc.  The descriptions of homework problems provide some guidance as to what is expected, but leave some of those choices up to you. Making such calls is an integral part of any data analysis project and we will be working on advancing this skill throughout
this course.

**Lastly -- do ask questions!  Piazza is the best for that**

# Banknote authentication (30 points)

This dataset presents an example of classification problem (authentic vs. counterfeit bank notes) using continuous predictors derived from image processing. More details about underlying data can be found in corresponding [dataset description](http://archive.ics.uci.edu/ml/datasets/banknote+authentication) at UCI ML website. To load data into R please use data file `data_banknote_authentication.txt` available at the course website as well as in UCI ML dataset repository.

Once the dataset in loaded into R, please name appropriately data set attributes, determine number of variables (explain which ones are predictors and which one is the outcome) and observations in the dataset (R functions such as `dim`, `nrow`, `ncol` could be useful for this), generate summary of the data using `summary` function in R and generate pairwise XY-scatterplots of each pair of continuous predictors indicating outcome using colour and/or shape of the symbols (you may find it convenient to use `pairs` plotting function). Describe your observations and discuss which of the variables are more likely to be informative with respect to discriminating forged bank notes from genuine.

Please comment on whether given the data at hand such problem appears to be an easy or hard one to solve.  Try to guess using your best intuition, what could be an error in discriminating forged banknotes from genuine  in this dataset: 50%, 20%, 10%, 5%, 2%, less than that?  Later in the course we will work with this dataset again to actually develop such a classifier at which point you will get quantitative answer to this question, but for now, just from visual inspection of the scatterplots above, what do you think such an error could be?  There is no wrong answer at this point, just try your best and make a note of it, so you can go back to it several weeks later.  Also, please discuss some of the tradeoffs that might be associated with that. For instance, should one expect the cost of calling genuine note counterfeit to be the same as making the opposite call (allowing fake bills as authentic).  Consider also anticipated frequency of these cases in the practical settings and how it could potentially interact with these costs.

```{r banknote}
data_path = './data/data_banknote_authentication.txt'
data = read.table(data_path, sep=',')
# * name appropriately data set attributes
columns = c('variance','skewness','curtosis','entropy','y_class')
colnames(data) <- columns

# * determine number of variables 
print(c('variable count: ', ncol(data)))

# * explain which ones are predictors and which one is the outcome
print(c('predictors are the first four columns:', columns[1:4]))
print(c('the outcome variable is remaining column:', columns[5]))

# * number of observations in the dataset (R functions such as dim, nrow, ncol could be useful for this)
print(c('observation count: ', nrow(data)))

# * generate summary of the data using summary function in R
print('summary of dataset')
summary(data)

# calculate overall pct
pct_y_class_one = format(round(100 * mean(data$y_class), 2), nsmall = 2)

# * generate pairwise XY-scatterplots of each pair of continuous predictors indicating outcome using colour and/or shape of the symbols
#       (you may find it convenient to use pairs plotting function). 
oldPar = par(mfrow=c(1:2),ps=16)
pairs(data[1:4], pch=19, cex=0.4, col=(data$y_class * -1 + 3), main='y_class: 0 = green, 1 = red')

```


### Answer:

The pairwise scatterplots, most obviously by any pair that includes the column I've labeled `variance`, seem to indicate detecting fraudulent banknotes, at least those generally matching sampled data, should be relatively easy. The `variance-vs-XYZ` plots each show distinc groupings of the two `y_class` values, where one can image a relatively simple line, even a close to straight line for some pairs, that clearly divides `y_class=0` from `y_class=1`. On the other hand, `skewness v. curtosis` and even moreso `curtosis v. entropy` scatter plots show much overlap between the two classes - where `variance` is probably the most significant predictor `entropy` might be the lease.
My guess at at an error rate would be something around 5-10%.  

One would think false negatives would be preferable to false positives for a problem like this, e.g. better to withdraw a potentially fraudulent banknote from circulation than to use a cutoff that results in more counterfeit notes in circulation. Part of detection will always be about cost and presumably there are more fine grained (expensive) methods available to further check banknotes for fraud, such as magnified visual inspection. These more costly methods can be used on any subset identified as likely-fraudulent, followed by permanently pulling out any bad ones. On the other hand, allowing too many fraudulent notes to be in circulation could undermine faith in the currency, not too mention encourage more counterfeiters to practice their trade as they are less likely to be caught.  

It would be easy enough to google the "assumed rate of fraudulent banknotes" for a given currency but I assume it is relatively low, though of course depending on the monetary unit, e.g. for U.S. currency the profit in counterfeiting $1 bills is low but doing same for $100 is more risky as they are much more likely to be closely examinded upon receipt. Assuming that 0 = non-fraudulent and 1 = is-fraudulent in the outcome variable (`y_class`) for this dataset, the percent of fraudulent notes, `r pct_y_class_one`% is much, much higher than that of real-world currencies.

# Abalone age (30 points)

This dataset presents an example of a regression problem -- predicting mollusc age from measurements that are easier to obtain. More details about underlying data can be found in corresponding [dataset description](http://archive.ics.uci.edu/ml/datasets/Abalone). To load data into R please use file `abalone.data` available at the course website as well as at the UCI ML data repository.

Once the dataset in loaded into R, please name appropriately data set attributes, determine number of variables (explain which ones are predictors -- categorical vs. continuous -- and which one is the outcome) and observations in the dataset (R functions such as `dim`, `nrow`, `ncol` could be useful for this), generate summary of the data using `summary` function in R and generate pairwise XY-scatterplots of each pair of *continuous* attributes.

Describe your observations and discuss which attributes might be more useful for predicting snail age.

For **extra 5 points** for some of the continuous and
categorical predictors generate boxplots rendering relationship between them.

```{r abalone1}
data_path = './data/abalone.data'
data = read.table(data_path, sep=',')

# Sex / nominal / -- / M, F, and I (infant)
# Length / continuous / mm / Longest shell measurement
# Diameter / continuous / mm / perpendicular to length
# Height / continuous / mm / with meat in shell
# Whole weight / continuous / grams / whole abalone
# Shucked weight / continuous / grams / weight of meat
# Viscera weight / continuous / grams / gut weight (after bleeding)
# Shell weight / continuous / grams / after being dried
# Rings / integer / -- / +1.5 gives the age in years 

# * name appropriately data set attributes
columns = c('sex','len','diameter','height','whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'y_rings')
colnames(data) = columns

# * determine number of variables 
print(c('variable count: ', ncol(data)))

# * explain which ones are predictors (- categorical vs. continuous -) and which one is the outcome
print(c('predictors are the first eight columns:', columns[1:length(columns)-1]))
print(c('the outcome variable is remaining column:', columns[length(columns)]))
print(c('all of the predictors are continuous variables save "sex"", which is categorical and can be of following values', as.character(unique(data$sex))))

# * number of observations in the dataset (R functions such as dim, nrow, ncol could be useful for this)
print(c('observation count: ', nrow(data)))

# * generate summary of the data using summary function in R
print('summary of dataset')
summary(data)

oldPar = par(mfrow=c(1:2),ps=16)

# plots are very small, take the 1 - 29 range of values for y_rings and divide into only 3 bins for visibility
palette(rainbow(3, alpha = 0.5))
pairs(data[2:8], pch=19, cex=0.4, col=(ceiling(data$y_rings/5)), main='colored by y_rings value, three bins')
```

**Above**  
There are 7 continuous variables, so pairwise scatterplots among those results in rather small plots. Therefore the `y_rings` values, which range from 1 to 29, have been divided into only 3 bins for greater visibility, i.e. young, middle-age, old.

Even with above binning I needed to create a pdf in order to be able to zoom into each plot to really see in which ones there is a clear division in rings (age). From there it appears multiple pairwise plots involving both `len` (Longest shell measurement) and `diameter`  (perpendicular to length) will likely be good predictors.  
Below are larger plots for 2 other variables against `len` and also 2 other variables against `diameter`, with these 4 plots showing decent clustering of the young/middle-age/old values. Some of the variables appear to be highly correlated, e.g. `len` and `diameter` have strong positive correlation and for prediction purposes we might only want to include one of those.  
The final pair of plots are against `shucked_weight`, where we can these combinations are not good candidates as the `y_rings` values are scattered amongst themselves, no clear clusters.

```{r abalone2}

oldPar = par(mfrow=c(1:2),ps=16, oma=c(0,0,2,0))
plot(data$len, data$diameter, pch=19, cex=0.7, col=(ceiling(data$y_rings/5)))
plot(data$len, data$height, pch=19, cex=0.7, col=(ceiling(data$y_rings/5)))
title('len vs x,y, good', outer=TRUE)

plot(data$diameter, data$height, pch=19, cex=0.7, col=(ceiling(data$y_rings/5)))
plot(data$diameter, data$shucked_weight, pch=19, cex=0.7, col=(ceiling(data$y_rings/5)))
title('diameter vs x,y, good', outer=TRUE)
     
plot(data$shucked_weight, data$shell_weight, pch=19, cex=0.7, col=(ceiling(data$y_rings/5)))
plot(data$shucked_weight, data$viscera_weight, pch=19, cex=0.7, col=(ceiling(data$y_rings/5)))
title('shucked_weight vs x,y, NOT good', outer=TRUE)

#best
#- len vs. diam, height, whole_weight
#- diam vs. height, shucked_weight

#BAD
#shucked_weight vs. shell_weight

```


Finally, below are boxplots of all the continuous predictors, each time against `sex`, the single categorical variable in this dataset.
```{r abalone3}


oldpar=par(mfrow=c(2,2))
for ( i in 2:5 ) {
  boxplot(data[[i]] ~ data$sex,
  main=names(data)[i],col=c("green","blue","gold"))
}

for ( i in 6:8 ) {
  boxplot(data[[i]] ~ data$sex,
  main=names(data)[i],col=c("green","blue","gold"))
}

par(oldpar)
```

# Tibbles (extra 5 points)

Fluency in R (as any other programming language) involves ability to look up, understand and put to use as necessary functionality that has not been explored before.  One of relatively recent additions to R are so-called tibbles that can be seen as ["modern take on data frames"](https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html).  To earn extra points for this assignment, please look up tibble use and constrast their behavior to that of conventional data frame using one of the datasets you have already created above.  To earn all points available your solution must include more than one example of substantive difference (i.e. same kind of difference illustrated by two datasets counts as one).  Please also comment on why use of tibbles may result in more robust code.

### Answer

The first, relatively minor, difference is the default number of rows returned by a call to print. I won't bother wasting space by doing a `print` against the full dataframe object (`df_aba`) but the point is that simply printing the df without restriction will result in the all ~5k rows of the abalone data set being output, which is likely not what a user wants - 10 is good enough and that is what a tibble does by default.  


```{r tibble1}
data_path = './data/abalone.data'
df_aba = read.table( data_path, sep=',')
tb_aba = as_tibble(df_aba)

# all would print out
#print(df_aba)

# only 10 will print
print(tb_aba)
# restrict the dataframe so that only the first 10 print out
print(df_aba[1:10,])


```
A more important aspect of the printout is that the tibble includes a descriptive row under the columns that provides the data type of each column, e.g. the bottom row in the following snippet:
```
## # A tibble: 4,177 x 9
##    V1       V2    V3     V4    V5     V6     V7     V8    V9
##    <fct> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> <int>
```

That datatype header line also points out another difference between dataframe and tibble, where the latter will not convert string columns as factors, while dataframes will do it by default upon loading the file. In my code above, I converted the dataframe into a tibble, so one can see the character column representing Sex had been automatically turned into a factor:
```
## # A tibble: 4,177 x 9
##    V1       V2    V3     V4    V5     V6     V7     V8    V9
##    <fct> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl> <int>
```

```{r tibble2}
tb = read_csv(data_path)
print(tb)
```

Reviewing the datatype line that comes from printing the results of a "raw" tibble illustrates:
1) that first column came in as a `<chr>`, i.e. character, instead of as a factor `<fct>`
2) the columns are not automatically named, in this case the first row of data were taken to be the names

```
   M     `0.455` `0.365` `0.095` `0.514` `0.2245` `0.101` `0.15`  `15`
  <chr>   <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>  <dbl> <int>
   ```

When column names are set for a dataframe certain "clean up" may occur, with the final column names being different than that originally supplied. This will not happen with a tibble. Below shows some of first columns having spaces or hyphens in them. When the dataframe version is instantiated the spaces and hyphens are changed to periods, while no such conversion happens with the tibble.

```{r tibble3}

columns = c('sex','length in mm','diameter in mm','height-mm','whole-weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'y_rings')
df_aba = read.table( data_path, sep=',', col.names = columns)
print(names(df_aba))
tb = read_csv(data_path, col_names = columns)
print(names(tb))
```

The above differences should result in more robust code:  
1) the print difference was somewhat minor but in rapid fire data exploration it can actually be quite annoying to have so many lines print out that you lose the visiable console history. It's still available of course but having to dip into history or scroll up on the screen can easily break one's train of thought.  
2) the lack of automatic coercion of string values into factors may not mean too much to an experienced R programmer who might have `stringsAsFactors = FALSE` ingrained in their consciousness but to someone newer to the language the automatic conversion can be quite tricky.  
3) column names should generally always be specified without spaces or hyphens but even worse is having R automatically change the values without any warning. Once can easily imagine an automated scenario where the column names are set in a header on an external file and it might just be easiest to use what is supplied and have code that quotes all columns by default. Such code could easily break if the column names are silently updated to different values.

