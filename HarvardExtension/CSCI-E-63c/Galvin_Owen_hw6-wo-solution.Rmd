---
title: 'CSCI E-63C: Week 6 Assignment'
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
options(scipen=5)

knitr::opts_chunk$set(echo = TRUE)
```

# Preface

In this assignment we will exercise some of the unsupervised learning approaches on [2016 Global Health Observatory data](http://www.who.int/gho/publications/world_health_statistics/2016/en/).  It is available at that website in the form of [Excel file](http://www.who.int/entity/gho/publications/world_health_statistics/2016/whs2016_AnnexB.xls?ua=1), but its cleaned up version ready for import into R for further analyses is available at CSCI E-63C canvas course web site [whs2016_AnnexB-data-wo-NAs.txt](https://canvas.harvard.edu/files/4665505/download?download_frd=1).  The cleaning and reformatting included: merging data from the two parts of Annex B, reducing column headers to one line with short tags, removal of ">", "<" and whitespaces, conversion to numeric format and replacement of undefined values (as indicated by en-dash'es in the Excel) with corresponding averages of those attributes.  The code that was used to format merged data is shown at the end of this document for your reference only.  You are advised to save yourself that trouble and start from preformatted text file available at the course website as shown above.  The explicit mapping of short variable names to their full description as provided in the original file is available in Excel file [whs2016_AnnexB-reformatted.xls](https://canvas.harvard.edu/files/4665506/download?download_frd=1) also available on the course canvas page.  Lastly, you are advised to download a local copy of this text file to your computer and access it there (as opposed to relying on R ability to establish URL connection to canvas that potentially requires login etc.)

Short example of code shown below illustrates reading this data from a local copy on your computer (assuming it has been copied into current working directory of your R session -- `getwd()` and `setwd()` commands are helpful to find out what is it currently and change it to desired location) and displaying summaries and pairs plot of five (out of almost 40) arbitrary chosen variables.  This is done for illustration purposes only -- the problems in the assignment expect use of all variables in this dataset.

```{r WHS}
whsAnnBdatNum <- read.table("./data/whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whsAnnBdatNum[,c(1,4,7,10,17)])
pairs(whsAnnBdatNum[,c(1,4,7,10,17)])
```

In some way this dataset is somewhat similar to the `USArrests` dataset extensively used in ISLR labs and exercises -- it collects various continuous statistics characterizing human population across different territories.  It is several folds larger though -- instead of `r nrow(USArrests)` US states and `r ncol(USArrests)` attributes in `USArrests`, world health statistics (WHS) data characterizes `r nrow(whsAnnBdatNum)` WHO member states by `r ncol(whsAnnBdatNum)` variables.  Have fun!

The following problems are largely modeled after labs and exercises from Chapter 10 ISLR.  If anything presents a challenge, besides asking questions on piazza (that is always a good idea!), you are also encouraged to review corresponding lab sections in ISLR Chapter 10.

# Problem 1: Principal components analysis (PCA) (25 points)

The goal here is to appreciate the impact of scaling of the input variables on the result of the principal components analysis.  To that end, you will first survey means and variances of the attributes in this dataset (sub-problem 1a) and then obtain and explore results of PCA performed on data as is and after centering and scaling each attribute to zero mean and standard deviation of one (sub-problem 1b).


## Sub-problem 1a: means and variances of WHS attributes (5 points)

Compare means and variances of the *untransformed* attributes in the world health statisics dataset -- plot of variance vs. mean is probably the best given the number of attributes in the dataset.  Function `apply` allows to apply desired function (e.g. `mean` or `var` or `sd`) to each row or column in the table.  Do you see all `r ncol(whsAnnBdatNum)` attributes in the plot, or at least most of them?  (Remember that you can use `plot(inpX,inpY,log="xy")` to use log-scale on both horizontal and vertical axes.)  Is there a dependency between attributes' averages and variances? What is the range of means and variances when calculated on untransformed data?  Which are the top two attributes with the highest mean or variance?  What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

---

> plot of variance vs. mean is probably the best given the number of attributes in the dataset.  


Use `plot` command on mean and variance of datasets variables, use log scale so that all values are discretely displayed. 
```{r p1a1}

# read dataset in again, usin a variable name I'm more comfortable with
data = read.table("./data/whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")


data_mean = apply(data, 2, mean)
print(data_mean)
data_var = apply(data, 2, var)
print(data_var)


# plot(inpX,inpY,log="xy")
plot(data_mean, data_var, log='xy', 
     xlab='mean (log scale)', ylab='variance (log scale)', 
     main='World Health Statistics 2016, variable means vs. variance')

# range(data_mean, 2)

```

> Is there a dependency between attributes' averages and variances?  

There does appear to be a dependency, a positive linear relationship, indicated by the positive slope of a theoretical line drawn through the data.

> What is the range of means and variances when calculated on untransformed data?

The individual mean values on the raw dataset range from a low of `r range(data_mean, 2)[1]` to a high of `r format(round(range(data_mean, 2)[2], 2), big.mark=',', scientific=FALSE)` while the range for variance is from `r range(data_var, 2)[1]` to `r format(round(range(data_var, 2)[2], 2), big.mark=',', scientific=FALSE)`. 

> Which are the top two attributes with the highest mean or variance? 

Two attributes with highest mean: 
```{r p1a2} 
format(data_mean[order(abs(data_mean), decreasing=TRUE)][1:2], scientific=FALSE)
```

Two attributes with highest variance: 
```{r p1a3} 
format(data_var[order(data_var, decreasing=TRUE)][1:2], scientific=FALSE)
```

> What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

Since the variables are measured in a variety of units, e.g.:  
- `TOTPOP` is total population in 000s, has high variance  
- `DEATHND` is average death rate due to natural disasters, per 100,000 population, low variance    
without scaling the variance from `TOTPOP` will essentially overwhelm most of the other variables in terms of loading vector. On the other end of the spectrum, the principle component loading vector for `DEATHND` will be very small, even if in a scaled environment it were to have a large variance, though scaled variance for this variable is speculative at the moment.  


## Sub-problem 1b: PCA on untransformed and scaled WHS data (20 points)

Perform the steps outlined below *both* using *untransformed* data and *scaled* attributes in WHS dataset (remember, you can use R function `prcomp` to run PCA and to scale data you can either use as input to `prcomp` the output of `scale` as applied to the WHS data matrix or call `prcomp` with parameter `scale` set to `TRUE`). To make it explicit, the comparisons outlined below have to be performed first on the unstransformed WHS data and then again on scaled WHS data -- you should obtain two sets of results that you could compare and contrast.

1. Obtain results of principal components analysis of the data (by using `prcomp`)
2. Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`)
3. Generate plot of the two first principal components using `biplot`.  Which variables seem to predominantly drive the results of PCA when applied to untransformed data?
  + Please note that in case of untransformed data you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?
4. The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.
  + What attributes have the largest (by their absolute value) loadings for the first and second principal component?
  + How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?
5. Calculate percentage of variance explained (PVE) by the first five principal components (PCs).  You can find an example of doing this in ISLR Chapter 10.4 (Lab 1 on PCA).

---

> 1. Obtain results of principal components analysis of the data (by using `prcomp`)
```{r p1b1}
pr_raw = prcomp(data, scale=FALSE)
pr_scaled = prcomp(data, scale=TRUE)

```

> 2. Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`)
```{r p1b2}
old.par = par(mfrow=c(1,2),ps=7)
plot(pr_raw, xlab='dimensions', main='RAW DATA: Variance explained by first several PC')
plot(pr_scaled, xlab='dimensions', main='SCALED: Variance explained by first several PC')
```

> 3. Generate plot of the two first principal components using `biplot`.  Which variables seem to predominantly drive the results of PCA when applied to untransformed data?

```{r fig.height = 10, fig.width = 10}
#biplot(pr_raw, scale=0)
# temporarily disable warnings make things less ugly on the untransformed plotting
options(warn=-1)
#pdf('raw_biplot.pdf')
biplot(pr_raw, pc.biplot=TRUE, main='untransformed data')
#dev.off()
options(warn=0)

biplot(pr_scaled, pc.biplot=TRUE, main='scaled data')

```

Plot the scaled data again, going back after figuring out how to add only select text labels:
```{r fig.height = 10, fig.width = 10}
#pr_scaled_rank_two = prcomp(data, scale=TRUE, rank.=2)
countries = c('Afghanistan','UnitedKingdom','UnitedStatesofAmerica','China', 'India', 'Mexico', 'Australia', 'Israel', 
              'Italy', 'Ireland','Sweden','Uganda','Chad','Honduras','SouthSudan','Cambodia')

biplot(pr_scaled, pc.biplot=TRUE, 
       xlabs=ifelse(rownames(pr_scaled$x) %in% countries, rownames(pr_scaled$x), '*'),
       main='scaled data biplot, with loadings')


```

For the untransformed data,`INTINTDS` alone appears to drive results of the first two Principle Components, it is actually the only variable I can read. Unsurprisingly this was, by far, the variable with the greatest variance. There are 36 warnings like `zero-length arrow is of indeterminate angle and so skipped`, so it appears the other 36 variables were skipped. `INTINTDS` is the header value for "Reported number of people requiring interventions against Neglected Tropical Diseases".


>   + Please note that in case of untransformed data you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?

As noted above there were 36 warnings = `zero-length arrow is of indeterminate angle and so skipped` in the plot of the untransformed data. These warnings are expected on the untransformed data insofar as the very high variance of `INTINTDS`, on an absolute scale, has wiped out the contributions of all other variables to the PCA model. That number is apparently reported in simple "number of people" and the country with the highest value, India, has almost 3x people in that value as that of second highest, Nigeria.


> 4. The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) > -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.

  
```{r p1b4}
#pr_raw$rotation[,order(abs(pr_raw$rotation[,1]), decreasing=TRUE  )]
#pr_raw$rotation[,1]

# untransformed rotation
pr_raw$rotation[,1:2]

# rotation after scaling
pr_scaled$rotation[,1:2]


```


>   + What attributes have the largest (by their absolute value) loadings for the first and second principal component?

For the untransformed data, largest loadings for first and second PC respectively are:
```{r p1b4b}
# untransformed rotation
sort(abs(pr_raw$rotation[,1]), decreasing=TRUE)[1]
sort(abs(pr_raw$rotation[,2]), decreasing=TRUE)[1]


```

For the scaled data, largest loadings for first and second PC respectively are:
```{r p1b4c}
# rotation after scaling
sort(abs(pr_scaled$rotation[,1]), decreasing=TRUE)[1]
sort(abs(pr_scaled$rotation[,2]), decreasing=TRUE)[1]


```

>   + How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?

Reiterating the two attributes from raw data with highest variance and mean: 
```{r p1b4d} 
format(data_var[order(data_var, decreasing=TRUE)][1:2], scientific=FALSE)

format(data_mean[order(abs(data_mean), decreasing=TRUE)][1:2], scientific=FALSE)

```
we can see the first PC of the raw data was primarily made up of the variable with the highest variance (and mean) = `INTINTDS`. Similarly the variable with the greatest contribution to the second PC, `TOTPOP`, is also the variable with the second highest variance/mean. (Comparing the scaled variance and means is not relevant as they have all been scaled to have mean = 0 and standard deviation = 1 ).


> 5. Calculate percentage of variance explained (PVE) by the first five principal components (PCs).  You can find an example of doing this in ISLR Chapter 10.4 (Lab 1 on PCA).


```{r p5a}
pve_raw = (pr_raw$sdev ^ 2)/sum(pr_raw$sdev ^ 2)
pct_raw_5 = format(pve_raw[1:5] * 100, scientific = FALSE)

pve_scaled = (pr_scaled$sdev ^ 2)/sum(pr_scaled$sdev ^ 2)
pct_scaled_5 = format(pve_scaled[1:5] * 100, scientific = FALSE)

```
The percentage of variance explained by the first five PCs of the raw data, in decreasng order: `r pct_raw_5`.  
Corresponding values for scaled data: `r pct_scaled_5`.

> Now that you have PCA results when applied to untransformed and scaled WHS data, please comment on how do they compare and what is the effect of scaling?  What dataset attributes contribute the most (by absolute value) to the top two principal components in each case (untransformed and scaled data)?  What are the signs of those contributions?  How do you interpret that?

Scaling has a huge effect on this particular dataset, as the variables are measured in a number of different units. On the raw data the `INTINTDS` values overwhelm everything else on PC 1. For PC2 on the untransformed data the most contributory variable  is `TOTPOP`. The signs for both attributes are positive, larger values for  "Reported number of people requiring interventions against Neglected Tropical Diseases" results in a positive PC1 contribution and similar for Total Population and PC2. 

For the scaled dataset, `LIFEXPB.F` is the biggest contributor, by absolute value, to the first Primary Component ( PC1) while `HOMICIDE` is the biggest contributor to the second Primary Component. The sign on `LIFEXPB.F` (female life expectancy at birth) is negative and sign on `HOMICIDE` is positive. Within a given PC this would mean a higher female life expectancy would be correlated with a lower homicide rate, not sure how that works across PCs.


> Please note, that the output of `biplot` with almost 200 text labels on it can be pretty busy and tough to read.  ? You can achieve better control when plotting PCA results if instead you plot the first two columns of the `x` attribute in the output of `prcomp` -- e.g. `plot(prcomp(USArrests,scale=T)$x[,1:2])`.  Then given this plot you can label a subset of countries on the plot by using `text` function in R to add labels at specified positions on the plot.  Please feel free to choose several countries of your preference and discuss the results.  Alternatively, indicate US, UK, China, India, Mexico, Australia, Israel, Italy, Ireland and Sweden and discuss the results. 

```{r fig.height = 8, fig.width = 8}

pr_scaled_2pc = pr_scaled$x[,1:2]
plot(pr_scaled_2pc, title='simple plot')

s = subset(pr_scaled_2pc, rownames(pr_scaled_2pc) %in% countries)
text(s, labels=rownames(s))

```

> Where do the countries you have plotted fall in the graph?  Considering what you found out about contributions of different attributes to the first two PCs, what do their positions tell us about their (dis-)similarities in terms of associated health statistics?

countries: `r countries`
There are several "first world"" countries in the lower left, where both PC1 and PC2 would be low. Again, female life expectancy has ~25% weighting (negative, so high female life results in low PC value) for PC1 and Homicide a ~39% rating for PC2. So these countries have women who live to longer ages and lower homicide rates. USA is at the higher edge of this "cluster", presumably because of a higher homicide rate than a country like Ireland or Sweden. Staying with higher female life expectancy and a higher homicide rate (though the former is only ~25% and even for PC2, there are many other attributes besides homicide), is a country like Mexico. There are some countries (South Sudan, Chad) in the lower right, with high PC1 but low PC2, so all else being equal a rather short female life exp. but also a low homicide rate. Most of the other labeled countries have a PC2 value similar to USA but varying values for PC2. Uganda and Afghanistan might be two that would be expected to have similarity in terms of various health statistics. The greatest dis-similarity might be between Sweded in the lower left and Lesotho, the unlabeled point toward the top, at around PC1 = 6.


# Problem 2: K-means clustering (20 points)

The goal of this problem is to practice use of K-means clustering and in the process appreciate the variability of the results due to different random starting assignments of observations to clusters and the effect of parameter `nstart` in alleviating it.

## Sub-problem 2a: k-means clusters of different size (5 points)

Using function `kmeans` perform K-means clustering on *explicitly scaled* (e.g. `kmeans(scale(x),2)`) WHS data for 2, 3 and 4 clusters.  Use `cluster` attribute in the output of `kmeans` to indicate cluster membership by color and/or shape of the corresponding symbols in the plot of the first two principal components generated independently on the same (scaled WHS) data.  E.g. `plot(prcomp(xyz)$x[,1:2],col=kmeans(xyz,4)$cluster)` where `xyz` is input data.  Describe the results.  Which countries are clustered together for each of these choices of $K$?

```{r p2a1}

set.seed(63)
km_2 = kmeans(scale(data), 2)
plot(prcomp(data, scale=TRUE)$x[,1:2], main='K-Means Clustering, K=2', col=km_2$cluster+1)
text(s, labels=rownames(s))

```

With K=2 there is essentially a vertical line drawn through the middle, bisecting the range of PC1. The highest contributor to PC1 was female life expectancy, with negative signage. It was only responsible for ~25% of PC1 but one would still expect the countries in the cluster on the right generally have a lower female life expectancy while those to the left have a higher value in `LIFEXPB.F`. Overall, the countries on the left, e.g. Sweden, U.K., seem to be wealthier than those on the right (Chad, Afghanistan).


```{r p2a2}
set.seed(63)
km_3 = kmeans(scale(data), 3)
plot(prcomp(data, scale=TRUE)$x[,1:2], main='K-Means Clustering, K=3', col=km_3$cluster+1)
text(s, labels=rownames(s))

```

With k=3 there continues to be a cluster division at a vertical line through PC1, and almost all the countries that were in the right hand cluster earlier remain in a cluster of approximately the same dimensions. On the left-hand side the countries that are generally considered to be more "first-world" are now in their own cluster to the far left while Mexico, China, et al. are in the middle cluster.

```{r p2a3}
set.seed(63)



km_4 = kmeans(scale(data), 4)
plot(prcomp(data, scale=TRUE)$x[,1:2], main='K-Means Clustering, K=4', col=km_4$cluster+1)
text(s, labels=rownames(s))

```

The cluster dynamics are now a little harder to decipher, and more specifically at K=4 the cluster definitions became more dynamic, encouraging me to `set.seed()` so that my text here had a hope of matching the plot. (I hadn't read 2b yet when i added set.seed... but might as keep it here I think). The orig cluster on the far right remains reasonably well defined. The "first-world" cluster from the previous plot is also similar to that generated by K=3. The cluster that had Mexico and Chine appears to have become softer, with an ill-defined new, smaller cluster cropping up at the top (countries that tend to have a higher homicide rate?).

## Sub-problem 2b: variability of k-means clustering and effect of `nstart` parameter (15 points)

By default, k-means clustering uses random set of centers as initial guesses of cluster centers.  Here we will explore variability of k-means cluster membership across several such initial random guesses.  To make such choices of random centers reproducible, we will use function `set.seed` to reset random number generator (RNG) used in R to make those initial guesses to known/controlled initial state.

Using the approach defined above, repeat k-means clustering of *explicitly scaled* WHS data with four (`centers=4`) clusters three times resetting RNG each time with `set.seed` using seeds of 1, 2 and 3 respectively (and default value of `nstart=1`).  Indicate cluster membership in each of these three trials on the plot of the first two principal components using color and/or shape as described above.  Two fields in the output of `kmeans` -- `tot.withinss` and `betweenss` -- characterize within and between clusters sum-of-squares. 

```{r p2b1}

nstart1 = matrix(0, ncol = 4, nrow = 3)
colnames(nstart1) = c('seed','tot.withinss','betweenss','ratio')

for (i in 1:3){
  set.seed(i)
  km = kmeans(scale(data), centers=4, nstart=1)
  
  nstart1[i,] = c(i,km$tot.withinss, km$betweenss, km$tot.withinss/km$betweenss)
  plot(prcomp(data, scale=TRUE)$x[,1:2], 
       main=sprintf('K=4, seed=%d, tot.withinss=%f, betweenss=%f, nstart=1', i, km$tot.withinss, km$betweenss), 
       cex.main=0.9,
       col=km$cluster+1)

}
  
nstart1
```

>  Tighter clustering results are those which have smaller ratio of within to between sum-of-squares.  What are the resulting ratios of within to between sum-of-squares for each of these three k-means clustering results (with random seeds of 1, 2 and 3)?

Above plots were with `nstart`=1, the `ratio` column of the `nstart1` matrix indicates the ratios of "within"" to "between"" sum-of-squares at each of the seed levels. Apparently `set.seed=1` is generating the tighest clusters of the three options.


> Please bear in mind that the actual cluster identity is assigned randomly and does not matter -- i.e. if cluster 1 from the first run of `kmeans` (with random seed of 1) and cluster 4 from the run with the random seed of 2 contain the same observations (country/states in case of WHS dataset), they are *the same* clusters.

> Repeat the same procedure (k-means with four clusters for RNG seeds of 1, 2 and 3) now using `nstart=100` as a parameter in the call to `kmeans`.  Represent results graphically as before.  


```{r p2b2}


nstart100 = matrix(0, ncol = 4, nrow = 3)
colnames(nstart100) = c('seed','tot.withinss','betweenss','ratio')
  
for (i in 1:3){
  set.seed(i)
  km = kmeans(scale(data), centers=4, nstart=100)
  nstart100[i,] = c(i,km$tot.withinss, km$betweenss, km$tot.withinss/km$betweenss)
  
  plot(prcomp(data, scale=TRUE)$x[,1:2], 
       main=sprintf('K=4, seed=%d, tot.withinss=%f, betweenss=%f, nstart=100', i, km$tot.withinss, km$betweenss), 
       cex.main=0.9,
       col=km$cluster+1)
}
  
nstart100
```
> How does cluster membership compare between those three runs now?  What is the ratio of within to between sum-of-squares in each of these three cases?  What is the impact of using higher than 1 (default) value of `nstart`?  What is the ISLR recommendation on this offered in Ch. 10.5.1?

The membership of each of the four clusters is now static, i.e. is not changing with different seed levels as happened with `nstart=1`. The contents of `nstart100` show the ratios of "within"" to "between"" sum-of-squares to be exactly the same (underlying values are the same also) at each of the seed levels - and the ratio value matches the "best" one derived by the earlier `nstart=1` plots. By using a higher `nstart`, there are `nstart` number of random assignments when initial clusters are created, with only the best results being used.  ISLR recommends, nay _strongly_ recommends, that large `nstart` value be used, e.g. 20 or 50.


> One way to achieve everything this sub-problem calls for is to loop over `nstart` values of 1 and 100, for each value of `nstart`, loop over RNG seeds of 1, 2 and 3, for each value of RNG seed, reset RNG, call `kmeans` and plot results for each combination of `nstart` and RNG seed value.

Pretty obvious by now I chose to loop the `set.seed` values but do separate blocks for `nstart=1`/`nstart=100`

# Problem 3: Hierarchical clustering (15 points)

## Sub-problem 3a: hierachical clustering by different linkages (10 points)

Cluster country states in (scaled) world health statistics data using default (Euclidean) distance and "complete", "average", "single" and "ward" linkages in the call to `hclust`.  Plot each clustering hierarchy, 


```{r  fig.height = 10, fig.width = 12}

hc_complete = hclust(dist(scale(data)), method='complete')
plot(hc_complete, main='complete linkage', xlab='', ylab='', sub='', cex=.8)

hc_average = hclust(dist(scale(data)), method='average')
plot(hc_average, main='average linkage', xlab='', ylab='', sub='', cex=.8)

hc_single = hclust(dist(scale(data)), method='single')
plot(hc_single, main='single linkage', xlab='', ylab='', sub='', cex=.8)

hc_ward = hclust(dist(scale(data)), method='ward.D')
plot(hc_ward, main='ward.D linkage', xlab='', ylab='', sub='', cex=.8)


```

There wasn't a snippet re use of `prcomp(xyz)$x[,1:2]` in this question, so I went ahead with `scale(data)`.

> describe the differences.  

Among the four, Complete and Average resemble one another, as compared to any other pairing. But they still don't look two similar and even after expanding the figure size in R and trying to examine the values in a third-party image editing tool I can't read the vast majority of country names. It does look like several of the same countries are not merged until rather high up in the tree, e.g. India, Syrian/ArabianRepublic, China, Nepal.  

The Single linkage tree also has many of those same countries maintaining their own clusters until higher in the tree. Beyond that it looks like many Western European countries like Sweden and the UK merge early on, toward the lower right.  Other countries then merge in at a regular rate, I can see Russia and the U.S. in the middle there.  

There are two notable features of Ward.D results, the first being that the early cluster merging at the bottom indicates many countries are considered similar to at least some other countries. There aren't the late-joiners like India and Nepal in the other models. Then of course there are the the two large clusters that maintain their separation until relatively late - it seems likely "two" would be a good number of clusters to select here.

> For comparison, plot results of clustering *untransformed* WHS data using default parameters (Euclidean distance, "complete" linkage) -- discuss the impact of the scaling on the outcome of hierarchical clustering.

```{r  fig.height = 10, fig.width = 12}

hc_complete_raw = hclust(dist(data), method='complete')
plot(hc_complete_raw, main='complete linkage - untransformed data', xlab='', ylab='', sub='', cex=.8)

```

> discuss the impact of the scaling on the outcome of hierarchical clustering.

Scaling has allowed for a large number of unique clusters with varying numbers of members to be created while with the unstransformed data almost all the countries merge right away, or at least very soon, into a small number of clusters. Presumably there are extremes in the Euclidian distance such that a country with with an unscaled high value, e.g. India in Total Population and other measures that directly depend on underlying population, will drive many other countries to merge since they are, in relative numerical sense, closer.

## Sub-problem 3b: compare k-means and hierarchical clustering (5 points)

Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and "complete" linkage.  Use function `table` to compare membership of these clusters to those produced by k-means clustering with four clusters in the Problem 2(b) when using `nstart=100` (and any of the RNG seeds) above.  Discuss the results.

```{r  }

hc_complete = hclust(dist(scale(data)), method='complete')
#plot(hc_complete, main='complete linkage', xlab='', ylab='', sub='', cex=.8)

hc4_clusters = cutree(hc_complete, k=4)

set.seed(2)
km4 = kmeans(scale(data), centers=4, nstart=100)
km4_clusters = km4$cluster
table(hc4_clusters, km4_clusters )


```

Above is the table produced by comparing cluster membership. If the clusters between the two models were a 100% match one would expect 4 cells with relatively large, non-zero numbers, each appearing once in rows 1 to 4 and once in columns 1 to 4, with all remaining values = 0. With a "very good" match there might be some smaller non-zero numbers in a few cells, particular countries that were put into two different clusters by the separate models. Nether is not quite what we see here, there are indeed some significant numbers but row 1 has two larger numbers and row 2 also has 2 large ones. Not sure if this was expected but basically there is a notable mismatch between the models in terms of cluster membership and in fact the heirarchical model seems to have two single member clusters.  
All rather difficult to express in words, easiest is to produce plots with points colored according to cluster membership, first K-Means and then Heirarchical Complete:

```{r }

plot(prcomp(data, scale=TRUE)$x[,1:2], col=km4_clusters+1, main='K-Means = 4')

s2 = subset(pr_scaled_2pc, rownames(pr_scaled_2pc) %in% c('India','SyrianArabRepublic'))
plot(prcomp(data, scale=TRUE)$x[,1:2], col=hc4_clusters+1, main='heirarchical complete')
text(s2, labels=rownames(s2), pos=4)

```

I've deliberately added text labels to those two lonely clusters in the second plot, each of those has only a single country within. Seems kind of strange but re-doing the dendogram and introducing `rect.hclust` to try to spec out the four clusters confirms that India and SyrianArabRepublic, all the way on the top left, were put into their own clusters.

```{r  fig.height = 8, fig.width = 8}
hc_complete = hclust(dist(scale(data)), method='complete')
plot(hc_complete, main='complete linkage', xlab='', ylab='', sub='', cex=.8)
rect.hclust(hc_complete, k=4)
```


# Appendix: pre-processing of WHS data

For your reference only -- the file it has generated is already available at our course website

```{r WHSpreproc,eval=FALSE}
whsAnnBdat <- read.table("../data/whs2016_AnnexB-data.txt",sep="\t",header=T,as.is=T,quote="")
dim(whsAnnBdat)
whsAnnBdat <- apply(whsAnnBdat,2,function(x)gsub(">","",gsub("<","",gsub(" ","",x))))
whsAnnBdat <- apply(whsAnnBdat,2,function(x){x[x==rawToChar(as.raw(150))]<-"";x})
rownames(whsAnnBdat) <- whsAnnBdat[,1]
whsAnnBdat <- whsAnnBdat[,-1]
whsAnnBdatNum <- apply(whsAnnBdat,2,as.numeric)
whsAnnBdatNum <- apply(whsAnnBdatNum,2,function(x){x[is.na(x)] <- mean(x,na.rm = TRUE);x})
rownames(whsAnnBdatNum) <- rownames(whsAnnBdat)
write.table(whsAnnBdatNum,"../data/whs2016_AnnexB-data-wo-NAs.txt",quote=F,sep="\t")
```
